import sys
import os
import json
import uuid
import argparse
import collections
from datetime import datetime

# AUTO-DEPENDENCY
try:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
except ImportError:
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "ebooklib", "beautifulsoup4"])
    import ebooklib
    from ebooklib import epub

# CONFIG
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
REPORT_DIR = os.path.join(BASE_DIR, "Library_Scan", "Processed_Reports")
MEDIA_DIR = os.path.join(BASE_DIR, "media_storage")

class CanonArchitect:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        self.book = epub.EpubBook()
        self.spine = ['nav']
        self.css = self._get_css()

    def _get_css(self):
        return """
        body { font-family: 'Times New Roman', serif; }
        h1 { border-bottom: 2px solid #333; text-align: center; }
        h2 { background-color: #eee; padding: 5px; }
        .entry { margin-bottom: 20px; border-left: 3px solid #ccc; padding-left: 10px; }
        .meta { color: #666; font-style: italic; font-size: 0.9em; }
        .timeline-item { margin-bottom: 10px; }
        .date { font-weight: bold; }
        img { max-width: 100%; border: 1px solid #333; }
        .gallery-grid { display: flex; flex-wrap: wrap; }
        """

    def build_encyclopedia(self, series_name, nodes):
        """
        nodes: Dict of { node_id: node_data } from Knowledge Graph.
        """
        file_name = f"{series_name} [Canon Encyclopedia].epub"
        out_path = os.path.join(self.output_dir, file_name)
        
        # 1. CHECK EXISTING
        if os.path.exists(out_path):
            print(f"[Prompt] Canon Source '{file_name}' exists.")
            # In a GUI/Web context, we'd ask. In CLI, we assume 'Update/Rebuild'.
            print("   > Rebuilding to include latest knowledge...")

        # SETUP BOOK
        self.book.set_identifier(str(uuid.uuid4()))
        self.book.set_title(f"The {series_name} Encyclopedia")
        self.book.set_language('en')
        self.book.add_author("Librarian AI")
        
        # Add CSS
        nav_css = epub.EpubItem(uid="style_nav", file_name="style/nav.css", media_type="text/css", content=self.css)
        self.book.add_item(nav_css)

        # 2. CATEGORIZE DATA
        categories = {
            "PEOPLE": [], "LOCATIONS": [], "THINGS": [], "EVENTS": [], "CONCEPTS": []
        }
        
        # Visuals accumulator
        visuals = collections.defaultdict(list) # { "Book Title": [img_path...] }

        for nid, n in nodes.items():
            # Sort into buckets
            n_type = n.get('type', 'UNKNOWN')
            
            # Map Parser Types to Encyclopedia Sections
            if "NARRATIVE" in n_type or "PERSON" in n.get('category', '').upper():
                categories["PEOPLE"].append(n)
            elif "LOCATION" in n.get('category', '').upper() or "SITE" in n_type:
                categories["LOCATIONS"].append(n)
            elif "TECHNICAL" in n_type or "ARTIFACT" in n.get('category', '').upper():
                categories["THINGS"].append(n)
            elif "EVENT" in n_type or n.get('timeline'):
                categories["EVENTS"].append(n)
            elif "RELIGIOUS" in n_type or "CONCEPT" in n_type:
                categories["CONCEPTS"].append(n)
            else:
                # Default bucket based on name analysis?
                categories["THINGS"].append(n)

            # Collect Images
            if n.get('images'):
                # Find source book
                src = n['sources'][0] if n['sources'] else "Unknown Source"
                for img in n['images']:
                    visuals[src].append((n['name'], img))

        # 3. BUILD SECTIONS
        toc_links = []

        # --- A. ENTITY SECTIONS ---
        for cat_name, items in categories.items():
            if not items: continue
            
            # Sort A-Z
            items.sort(key=lambda x: x['name'])
            
            # Create Section File
            filename = f"sect_{cat_name.lower()}.xhtml"
            content = f"<h1>{cat_name}</h1>"
            
            for item in items:
                # Format Entry
                first_seen = item['sources'][0] if item['sources'] else "Unknown"
                desc = item.get('description', 'No detailed record.')
                
                content += f"""
                <div class="entry">
                    <h3>{item['name']}</h3>
                    <div class="meta">First Mentioned: {first_seen}</div>
                    <p>{desc}</p>
                """
                
                # Tech Specs
                if item.get('specs'):
                    content += "<ul>" + "".join([f"<li>{s}</li>" for s in item['specs']]) + "</ul>"
                
                # Timeline References (Personal History)
                if item.get('timeline'):
                    content += "<h4>History</h4><ul>"
                    for t in item['timeline'][:5]: # Limit to top 5 to save space
                        content += f"<li><b>{t.get('date','?')}</b>: {t.get('action','')}</li>"
                    content += "</ul>"
                
                content += "</div><hr/>"

            # Add to Book
            chap = epub.EpubHtml(title=cat_name.title(), file_name=filename, content=content)
            chap.add_item(nav_css)
            self.book.add_item(chap)
            self.spine.append(chap)
            toc_links.append(chap)

        # --- B. MASTER TIMELINE ---
        # Aggregate all timelines from all nodes
        all_events = []
        for n in nodes.values():
            for t in n.get('timeline', []):
                # Try to parse date
                d_str = t.get('date', 'Unknown')
                # Simple sort key (try to find year)
                import re
                year_match = re.search(r'\d{4}', str(d_str))
                year = float(year_match.group()) if year_match else 99999
                
                all_events.append({
                    "year": year,
                    "date_str": d_str,
                    "action": t.get('action', ''),
                    "entity": n['name'],
                    "snippet": t.get('snippet', '')
                })
        
        all_events.sort(key=lambda x: x['year'])
        
        if all_events:
            t_content = "<h1>Chronological History</h1>"
            for e in all_events:
                if e['year'] != 99999:
                    t_content += f"""
                    <div class="timeline-item">
                        <span class="date">{e['date_str']}</span> - 
                        <b>{e['entity']}</b>: {e['action']}
                        <br/><small>{e['snippet']}</small>
                    </div>
                    """
            
            time_chap = epub.EpubHtml(title="Timeline", file_name="timeline.xhtml", content=t_content)
            time_chap.add_item(nav_css)
            self.book.add_item(time_chap)
            self.spine.append(time_chap)
            toc_links.append(time_chap)

        # --- C. VISUAL GLOSSARY ---
        if visuals:
            g_content = "<h1>Visual Glossary</h1>"
            for book_title, imgs in visuals.items():
                g_content += f"<h2>From: {book_title}</h2>"
                for entity_name, img_rel_path in imgs:
                    # We need to embed the image file into the EPUB
                    # Image path is likely 'media_storage/filename.jpg'
                    real_path = os.path.join(BASE_DIR, img_rel_path.strip("/"))
                    
                    if os.path.exists(real_path):
                        img_name = os.path.basename(real_path)
                        try:
                            # Read image binary
                            with open(real_path, 'rb') as f: img_data = f.read()
                            
                            # Create EpubImageItem
                            epub_img = epub.EpubItem(
                                uid=img_name, 
                                file_name=f"images/{img_name}", 
                                media_type="image/jpeg", 
                                content=img_data
                            )
                            self.book.add_item(epub_img)
                            
                            # Add to HTML
                            g_content += f"""
                            <div style="text-align:center; margin-bottom:20px;">
                                <img src="images/{img_name}" style="max-height:300px;"/><br/>
                                <b>{entity_name}</b>
                            </div>
                            """
                        except: pass
            
            gal_chap = epub.EpubHtml(title="Visual Glossary", file_name="gallery.xhtml", content=g_content)
            gal_chap.add_item(nav_css)
            self.book.add_item(gal_chap)
            self.spine.append(gal_chap)
            toc_links.append(gal_chap)

        # FINALIZE
        self.book.toc = toc_links
        self.book.add_item(epub.EpubNcx())
        self.book.add_item(epub.EpubNav())
        self.book.spine = self.spine
        
        epub.write_epub(out_path, self.book, {})
        print(f"[Canon] Encyclopedia built: {out_path}")
        return out_path

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("series", help="Name of series to build canon for")
    args = parser.parse_args()
    
    # LOAD DATA
    print(f"[Canon] Loading Knowledge Graph for {args.series}...")
    
    # Strategy: Load ALL available knowledge graphs, then filter by Series
    # Since Knowledge Graphs might be per-book, we aggregate.
    
    aggregated_nodes = {}
    
    # 1. Scan Metadata to find books in this series
    target_files = []
    for root, dirs, files in os.walk(os.path.join(BASE_DIR, "Library_Scan")):
        for f in files:
            if f.endswith(".meta.json"):
                try:
                    with open(os.path.join(root, f), 'r', encoding='utf-8') as jf:
                        meta = json.load(jf)
                        if meta.get('series') and args.series.lower() in meta['series'].lower():
                            # This book is in series. Look for its graph.
                            # Graph is usually named Title_knowledge_graph.json in Processed_Reports
                            title = meta.get('title', 'Unknown')
                            graph_path = os.path.join(REPORT_DIR, f"{title}_knowledge_graph.json")
                            if os.path.exists(graph_path):
                                target_files.append(graph_path)
                except: pass

    # 2. Merge Graphs
    for p in target_files:
        try:
            with open(p, 'r', encoding='utf-8') as jf:
                data = json.load(jf)
                nodes = data.get("nodes", {})
                for nid, n in nodes.items():
                    if nid not in aggregated_nodes:
                        aggregated_nodes[nid] = n
                    else:
                        # Merge logic (append timelines, sources)
                        aggregated_nodes[nid]['sources'].extend(n['sources'])
                        if 'timeline' in n:
                            aggregated_nodes[nid].setdefault('timeline', []).extend(n['timeline'])
        except: pass

    if not aggregated_nodes:
        print("[Error] No knowledge data found for this series.")
    else:
        builder = CanonArchitect(REPORT_DIR)
        builder.build_encyclopedia(args.series, aggregated_nodes)
