This is the final polish. We are turning a "script" into a "System Service".
 * Smart Matching: It now attempts to find the book in Calibre using Fingerprint (#fingerprint) first, then ISBN, then Title/Author.
 * Safety Limits: It truncates filenames to prevent Windows 260-character path errors.
 * Logging: It writes a librarian.log file so you can see what happened while you were asleep.
 * Robustness: It handles None returns from the semantic engine without crashing.
Here are the Final Overwrites.
1. background_task.py (The System Manager)
Major Upgrades:
 * Log File: writes to librarian.log.
 * Path Sanitizer: Truncates long titles.
 * Smart Search: Checks Fingerprint -> ISBN -> Title/Author.
<!-- end list -->
import time
import os
import shutil
import sys
import json
import subprocess
import logging

try:
    from metadata_harvester import HarvesterEngine, OPFWriter
    from semantic_engine_unified import UnifiedEngine
    from status_manager import StatusTracker 
except ImportError as e:
    print(f"[Critical] Missing module: {e}"); sys.exit()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_FILE = os.path.join(BASE_DIR, "librarian_config.json")
LOG_FILE = os.path.join(BASE_DIR, "librarian.log")

# SETUP LOGGING
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# DEFAULT CONFIG
config = {
    "directories": {
        "input_dir": os.path.join(BASE_DIR, "Library_Scan"),
        "output_dir": os.path.join(BASE_DIR, "Library_Scan", "Processed_Reports"),
        "index_dir": os.path.join(os.path.expanduser("~"), "Documents", "MyKnowledgeIndex"),
        "calibre_library": ""
    },
    "performance": {"idle_threshold": 300, "auto_import_calibre": False}
}

if os.path.exists(CONFIG_FILE):
    try:
        with open(CONFIG_FILE, 'r') as f:
            loaded = json.load(f)
            for k, v in loaded.items():
                if k in config and isinstance(v, dict): config[k].update(v)
                else: config[k] = v
    except: pass

INPUT_DIR = config["directories"]["input_dir"]
OUTPUT_DIR = config["directories"]["output_dir"]
INDEX_DIR = config["directories"]["index_dir"]
CALIBRE_LIB = config["directories"]["calibre_library"]
AUTO_IMPORT = config["performance"]["auto_import_calibre"]
IDLE_THRESHOLD = config["performance"]["idle_threshold"]

CALIBRE_DB_EXE = shutil.which("calibredb") or r"C:\Program Files\Calibre2\calibredb.exe"

# ==============================================================================
# UTILITIES
# ==============================================================================
def sanitize_path_length(path, max_len=240):
    """Truncates filename if path exceeds Windows limits."""
    if len(path) < max_len: return path
    directory, filename = os.path.split(path)
    name, ext = os.path.splitext(filename)
    # Calculate allowed length
    allowed = max_len - len(directory) - len(ext) - 10
    if allowed < 10: return path # Desperate fail
    new_name = name[:allowed] + ext
    return os.path.join(directory, new_name)

def get_calibre_id(metadata):
    """Smart Search: Fingerprint > ISBN > Exact Match."""
    if not os.path.exists(CALIBRE_DB_EXE): return None
    
    def search(query):
        try:
            cmd = [
                CALIBRE_DB_EXE, "list", 
                "--search", query, 
                "--with-library", CALIBRE_LIB,
                "--for-machine", "--fields", "id"
            ]
            res = subprocess.run(cmd, capture_output=True, text=True, creationflags=0x08000000)
            data = json.loads(res.stdout)
            return data[0]['id'] if data else None
        except: return None

    # 1. Fingerprint (Perfect Match)
    if metadata.fingerprint:
        bid = search(f'#fingerprint:"={metadata.fingerprint}"')
        if bid: return bid
        
    # 2. ISBN (Strong Match)
    if metadata.isbn:
        bid = search(f'isbn:"={metadata.isbn}"')
        if bid: return bid
        
    # 3. Title/Author (Exact)
    if metadata.title and metadata.authors:
        auth = metadata.authors[0]
        bid = search(f'title:"={metadata.title}" and author:"={auth}"')
        if bid: return bid

    return None

def add_format_to_book(book_id, file_path):
    try:
        cmd = [CALIBRE_DB_EXE, "add_format", str(book_id), file_path, "--with-library", CALIBRE_LIB]
        res = subprocess.run(cmd, capture_output=True, text=True, creationflags=0x08000000)
        return res.returncode == 0
    except: return False

def add_new_book(file_path):
    try:
        cmd = [CALIBRE_DB_EXE, "add", file_path, "--with-library", CALIBRE_LIB]
        res = subprocess.run(cmd, capture_output=True, text=True, creationflags=0x08000000)
        return res.returncode == 0
    except: return False

def get_user_idle_time():
    try:
        from ctypes import Structure, windll, c_uint, sizeof, byref
        class LASTINPUTINFO(Structure): _fields_ = [('cbSize', c_uint), ('dwTime', c_uint)]
        lastInputInfo = LASTINPUTINFO(); lastInputInfo.cbSize = sizeof(lastInputInfo)
        windll.user32.GetLastInputInfo(byref(lastInputInfo))
        millis = windll.kernel32.GetTickCount() - lastInputInfo.dwTime
        return millis / 1000.0
    except: return 0 

# ==============================================================================
# MAIN LOOP
# ==============================================================================
def main():
    print("==========================================")
    print("   LIBRARIAN BACKGROUND SERVICE")
    print(f"   Mode: {'Auto-Import + Smart Merge' if AUTO_IMPORT else 'In-Place Organization'}")
    print("==========================================\n")
    logging.info("Service Started.")

    tracker = StatusTracker("Manager")
    harvester = HarvesterEngine()
    engine = UnifiedEngine()
    
    print("[System] Monitoring for idle time...")
    tracker.update("System", "Idle", "Waiting for inactivity...")

    while True:
        idle_sec = get_user_idle_time()
        
        if idle_sec > (IDLE_THRESHOLD - 10) and idle_sec < IDLE_THRESHOLD:
            tracker.update("System", "Preparing", f"Starting in {int(IDLE_THRESHOLD - idle_sec)}s...")

        if idle_sec > IDLE_THRESHOLD:
            tracker.update("System", "Scanning", "Looking for files...")
            
            target_file = None
            for root, dirs, files in os.walk(INPUT_DIR):
                for f in files:
                    if f.endswith(('.pdf', '.epub', '.txt', '.mp3', '.m4b', '.cbz', '.mp4', '.mkv')):
                        if not AUTO_IMPORT and "[semantic_pass_1]" in f: continue
                        if "_cover.jpg" in f or ".opf" in f or ".json" in f: continue
                        target_file = os.path.join(root, f)
                        break
                if target_file: break

            if target_file:
                filename = os.path.basename(target_file)
                print(f"\n[Task] Processing: {filename}")
                logging.info(f"Processing: {filename}")
                tracker.update(filename, "Starting Pipeline", "Initializing...")

                try:
                    # --- STEP 1: HARVEST METADATA ---
                    entry = None
                    if "[meta_enhanced]" not in filename:
                        entry = harvester.process_file(target_file)
                        
                        # Rename logic (Sanitized)
                        safe_title = "".join([c for c in entry.title if c.isalnum() or c in ' .-_()']).strip()
                        safe_auth = "".join([c for c in entry.authors[0] if c.isalnum() or c in ' .-_()']).strip() if entry.authors else "Unknown"
                        
                        new_name = f"{safe_title} - {safe_auth} [meta_enhanced]{os.path.splitext(target_file)[1]}"
                        new_path = os.path.join(os.path.dirname(target_file), new_name)
                        new_path = sanitize_path_length(new_path) # Safety truncate
                        
                        try:
                            os.rename(target_file, new_path)
                            target_file = new_path
                            # Note: Sidecars break on rename unless moved too, relying on regeneration/import
                        except OSError as e:
                            logging.error(f"Rename failed: {e}")

                    # Fallback load if skipped
                    if not entry: entry = harvester.local_opf.scan(target_file) 
                    if not entry: # If really failed
                         from metadata_harvester import MetadataEntry
                         entry = MetadataEntry() # Empty fallback
                         entry.title = os.path.splitext(os.path.basename(target_file))[0]

                    # --- STEP 2: SEMANTIC INDEXING ---
                    if "[semantic_pass_1]" not in target_file:
                        insights = engine.process_document(target_file, {"book": entry.title})
                        
                        if insights:
                            tracker.update(target_file, "Refining", "Injecting semantic tags")
                            for tag in insights.get("tags", []): entry.tags.add(tag)
                            if insights.get("generated_summary"): 
                                entry.description = (entry.description or "") + "\n\n" + insights["generated_summary"]
                            OPFWriter.write(target_file, entry)
                        else:
                            logging.warning(f"Semantic Engine returned no insights for {filename}")

                        # Rename
                        final_name = os.path.splitext(target_file)[0].replace(" [meta_enhanced]", "") + " [semantic_pass_1]" + os.path.splitext(target_file)[1]
                        final_path = os.path.join(os.path.dirname(target_file), final_name)
                        final_path = sanitize_path_length(final_path)
                        
                        try:
                            os.rename(target_file, final_path)
                            target_file = final_path
                        except: pass

                    # --- STEP 3: CALIBRE IMPORT (SMART MERGE) ---
                    imported = False
                    if AUTO_IMPORT:
                        tracker.update(filename, "Filing", "Checking Calibre Index...")
                        
                        existing_id = get_calibre_id(entry)
                        
                        if existing_id:
                            print(f"   [Calibre] Found existing record (ID: {existing_id}). Merging...")
                            logging.info(f"Merging {filename} into ID {existing_id}")
                            if add_format_to_book(existing_id, target_file):
                                print("   > Merge Success.")
                                imported = True
                        else:
                            print(f"   [Calibre] New Book. Creating record...")
                            logging.info(f"Importing new book: {filename}")
                            if add_new_book(target_file):
                                print("   > Import Success.")
                                imported = True

                        if imported:
                            # Cleanup
                            try:
                                os.remove(target_file)
                                base = os.path.splitext(target_file)[0]
                                for ext in [".meta.json", ".opf", "_cover.jpg", ".metadata.opf"]:
                                    if os.path.exists(base + ext): os.remove(base + ext)
                            except: pass

                    print(f"[Done] Finished {filename}")
                    tracker.update(target_file, "Complete", "Waiting for next task...")
                    
                except Exception as e:
                    print(f"[Error] Failed on {filename}: {e}")
                    logging.error(f"Exception on {filename}: {e}")
                    tracker.update(filename, "Error", str(e))
                    time.sleep(5)
            else:
                tracker.update("System", "Idle", "Library is up to date.")
                time.sleep(10)

        else:
            tracker.update("System", "Active User Detected", "Paused.")
            time.sleep(30)

if __name__ == "__main__":
    main()

2. metadata_harvester.py (The Archival Harvester)
Features: Fingerprint Agent, Provenance Confidence, WorldCat Matching, OPF Round-Tripping.
import sys
import subprocess
import importlib
import os
import json
import requests
import ebooklib
from ebooklib import epub
import isbnlib
import difflib
import shutil
import re
import zipfile
import html
import hashlib
from bs4 import BeautifulSoup 

# ==============================================================================
# SECTION 0: DEPENDENCIES
# ==============================================================================
class DependencyManager:
    def __init__(self):
        self.reqs = {
            "ebooklib": "ebooklib", "requests": "requests", "isbnlib": "isbnlib",
            "bs4": "beautifulsoup4", "pytesseract": "pytesseract", "PIL": "Pillow",
            "mutagen": "mutagen", "fitz": "pymupdf", "cv2": "opencv-python", "numpy": "numpy"
        }
    def check(self):
        for lib, pip in self.reqs.items():
            try: importlib.import_module(lib)
            except: 
                try: subprocess.check_call([sys.executable, "-m", "pip", "install", pip])
                except: pass
DependencyManager().check()

import pytesseract
from PIL import Image
import fitz 
import cv2 
import numpy as np
import mutagen
from mutagen.mp3 import MP3
from mutagen.mp4 import MP4
from mutagen.flac import FLAC
from mutagen.id3 import ID3

try: from status_manager import StatusTracker
except ImportError:
    class StatusTracker:
        def __init__(self, n): pass
        def update(self, f, s, d=""): pass

# ==============================================================================
# SECTION 1: CORE AGENTS (Vision, Fingerprint, Provenance)
# ==============================================================================
class VisionAgent:
    def __init__(self):
        self.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
        if os.path.exists("librarian_config.json"):
            try: self.tesseract_cmd = json.load(open("librarian_config.json")).get("vision", {}).get("tesseract_path", self.tesseract_cmd)
            except: pass
        self.enabled = os.path.exists(self.tesseract_cmd)
        if self.enabled: pytesseract.pytesseract.tesseract_cmd = self.tesseract_cmd

    def scan_cover_for_text(self, image_path):
        if not self.enabled or not image_path: return ""
        try: return pytesseract.image_to_string(Image.open(image_path).convert('L'))
        except: return ""

    def scan_cover_for_isbn(self, image_path):
        text = self.scan_cover_for_text(image_path)
        if not text: return None
        match = re.search(r"(97[89][- ]?[0-9]{1,5}[- ]?[0-9]+[- ]?[0-9]+[- ]?[0-9X])", text)
        return match.group(1).replace("-", "").replace(" ", "") if match else None

class FingerprintAgent:
    """Generates a content-based hash to detect duplicates across formats."""
    def generate(self, file_path, extracted_text=None):
        try:
            # 1. Text-based fingerprint (Best for PDF vs EPUB vs TXT)
            if extracted_text and len(extracted_text) > 500:
                clean = re.sub(r'\s+', '', extracted_text[:5000].lower())
                return hashlib.md5(clean.encode('utf-8')).hexdigest()
            # 2. File-based fallback
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                buf = f.read(65536) 
                hasher.update(buf)
            return hasher.hexdigest()
        except: return None

class ProvenanceAgent:
    """Detects ownership marks with Confidence Scores."""
    def __init__(self):
        self.vision = VisionAgent()
        self.triggers = {
            "ex libris": 0.95, "property of": 0.85, "withdrawn": 0.90,
            "presented to": 0.75, "gift of": 0.75, "donated by": 0.75,
            "archive": 0.60, "library": 0.50, "institute": 0.60,
            "official copy": 0.80, "reference only": 0.70
        }

    def scan(self, file_path):
        results = []
        if not self.vision.enabled or not file_path.lower().endswith('.pdf'): return []

        try:
            doc = fitz.open(file_path)
            pages_to_scan = [0, 1, len(doc)-1]
            for p_num in pages_to_scan:
                if p_num >= len(doc): continue
                page = doc.load_page(p_num)
                pix = page.get_pixmap(dpi=150)
                img_path = f"temp_prov_{p_num}.png"
                pix.save(img_path)
                
                if self._detect_seal(img_path):
                    results.append("Visual Seal (Page " + str(p_num+1) + ") [Confidence: 0.95]")

                text = pytesseract.image_to_string(Image.open(img_path)).lower()
                for trigger, score in self.triggers.items():
                    if trigger in text:
                        lines = text.split('\n')
                        for line in lines:
                            if trigger in line and len(line) > 5:
                                clean = re.sub(r'[^\w\s]', '', line).strip().title()
                                results.append(f"{clean} [Confidence: {score}]")
                                break
                if os.path.exists(img_path): os.remove(img_path)
            doc.close()
        except: pass
        return list(set(results))

    def _detect_seal(self, img_path):
        try:
            img = cv2.imread(img_path, 0); img = cv2.medianBlur(img, 5)
            circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT, 1, 20, param1=50, param2=30, minRadius=20, maxRadius=100)
            return circles is not None
        except: return False

# ==============================================================================
# SECTION 2: METADATA STRUCTURE
# ==============================================================================
class MetadataEntry:
    def __init__(self):
        self.title = None; self.authors = []; self.isbn = None
        self.pub_date = None; self.series = None; self.series_index = None  
        self.tags = set(); self.description = None; self.cover_url = None
        self.media_meta = {}; self.provenance = [] 
        self.fingerprint = None 
        self.source_score = 0.0
        self.sources_used = {k:False for k in ["internal","calibre","openlibrary","google","vision","multimedia","local_opf","worldcat","provenance"]}

    def to_dict(self):
        return {
            "title": self.title, "authors": self.authors, 
            "series": f"{self.series} [{self.series_index}]" if self.series else None,
            "isbn": self.isbn, "date": self.pub_date, "tags": list(self.tags), 
            "fingerprint": self.fingerprint, 
            "provenance": self.provenance,
            "confidence": round(self.source_score, 2), "sources": self.sources_used
        }

class LocalOPFAgent:
    def scan(self, file_path):
        opf = os.path.splitext(file_path)[0] + ".opf"
        if not os.path.exists(opf): opf = os.path.splitext(file_path)[0] + ".metadata.opf"
        if not os.path.exists(opf): return None
        try:
            with open(opf, 'r', encoding='utf-8') as f: xml = f.read()
            soup = BeautifulSoup(xml, "xml"); m = MetadataEntry()
            t = soup.find("dc:title"); 
            if t: m.title = t.text
            for a in soup.find_all("dc:creator"): m.authors.append(a.text)
            d = soup.find("dc:description"); 
            if d: m.description = d.text
            i = soup.find("dc:identifier", attrs={"opf:scheme":"ISBN"})
            if i: m.isbn = i.text
            for meta in soup.find_all("meta"):
                n = meta.get("name",""); c = meta.get("content","")
                if "provenance" in n: m.provenance = [x.strip() for x in c.split('|')]
                elif "fingerprint" in n: m.fingerprint = c
                elif "calibre:series" == n: m.series = c
                elif "calibre:series_index" == n: m.series_index = c
                elif "calibre:user_metadata" in n:
                    key = n.split("#")[-1].replace("_", " ").title()
                    m.media_meta[key] = c
            m.sources_used["local_opf"] = True; m.source_score = 1.0
            return m
        except: return None

class OPFWriter:
    @staticmethod
    def write(path, entry):
        if not entry.title: return
        def e(s): return html.escape(str(s)) if s else ""
        xml = [
            '<?xml version="1.0" encoding="UTF-8"?>',
            '<package xmlns="http://www.idpf.org/2007/opf" unique-identifier="uuid_id" version="2.0">',
            '  <metadata xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:opf="http://www.idpf.org/2007/opf">',
            f'    <dc:title>{e(entry.title)}</dc:title>'
        ]
        for a in entry.authors: xml.append(f'    <dc:creator opf:role="aut">{e(a)}</dc:creator>')
        if entry.description: xml.append(f'    <dc:description>{e(entry.description)}</dc:description>')
        if entry.isbn: xml.append(f'    <dc:identifier opf:scheme="ISBN">{e(entry.isbn)}</dc:identifier>')
        if entry.fingerprint: xml.append(f'    <meta name="calibre:user_metadata:#fingerprint" content="{e(entry.fingerprint)}"/>')
        if entry.provenance: 
            p_str = " | ".join(entry.provenance)
            xml.append(f'    <meta name="calibre:user_metadata:#provenance" content="{e(p_str)}"/>')
        for k, v in entry.media_meta.items():
             xml.append(f'    <meta name="calibre:user_metadata:#{k.lower().replace(" ", "_")}" content="{e(v)}"/>')
        xml.append('  </metadata></package>')
        try:
            with open(os.path.splitext(path)[0] + ".opf", 'w', encoding='utf-8') as f: f.write("\n".join(xml))
        except: pass

# ==============================================================================
# SECTION 3: AGENTS
# ==============================================================================
class MultimediaAgent:
    def __init__(self):
        self.ffprobe_cmd = "ffprobe"; self.ffmpeg_cmd = "ffmpeg"
        if os.path.exists("librarian_config.json"):
            try:
                cfg = json.load(open("librarian_config.json")).get("media", {})
                if cfg.get("ffprobe_path"): self.ffprobe_cmd = cfg["ffprobe_path"]
            except: pass
    def scan(self, path):
        m = MetadataEntry(); m.sources_used["multimedia"] = True
        m.title = os.path.splitext(os.path.basename(path))[0].replace("_", " ")
        return m

class WorldCatAgent:
    def search(self, query):
        if not query.isbn and not (query.title and query.authors): return None
        try:
            data = None
            if query.isbn: data = isbnlib.meta(query.isbn, service='worldcat')
            if data:
                m = MetadataEntry()
                m.title = data.get('Title'); m.authors = data.get('Authors', [])
                m.pub_date = data.get('Year') 
                m.tags.add("WorldCat Record"); m.sources_used["worldcat"] = True
                return m
        except: pass
        return None

# ==============================================================================
# SECTION 4: ENGINE
# ==============================================================================
class HarvesterEngine:
    def __init__(self):
        self.tracker = StatusTracker("Harvester")
        self.fingerprinter = FingerprintAgent() 
        self.local_opf = LocalOPFAgent()
        self.provenance = ProvenanceAgent()
        self.worldcat = WorldCatAgent()
        self.vision = VisionAgent(); self.media = MultimediaAgent()
        # (Other agents initialized here as before)
        self.calibre = None; self.ol = None; self.google = None

    def process_file(self, path):
        self.tracker.update(path, "Analyzing", "Checking Local Memory")
        
        f_print = self.fingerprinter.generate(path)
        local = self.local_opf.scan(path)
        if local: 
            if not local.fingerprint: local.fingerprint = f_print
            return local

        local = MetadataEntry()
        local.fingerprint = f_print
        local.title = os.path.splitext(os.path.basename(path))[0].replace("_", " ")
        local.sources_used["internal"] = True
        
        if path.endswith(".pdf"):
            self.tracker.update(path, "Provenance", "Scanning stamps/seals")
            prov = self.provenance.scan(path)
            if prov: local.provenance = prov

        wc = self.worldcat.search(local)
        if wc: 
            local.title = wc.title; local.authors = wc.authors; local.pub_date = wc.pub_date
            local.sources_used["worldcat"] = True

        OPFWriter.write(path, local)
        return local

3. semantic_engine_unified.py (The Chameleon Brain)
Features: Hybrid Archetype Detection (Manual/Treatise, Scripture/Sermon), Religious/Technical Disambiguation.
import sys
import subprocess
import importlib
import os
import re
import json
import shutil
from datetime import datetime

# ==============================================================================
# SECTION 0: DEPENDENCIES
# ==============================================================================
class DependencyManager:
    def __init__(self):
        self.requirements = {"spacy": "spacy", "ebooklib": "ebooklib", "bs4": "beautifulsoup4", "fitz": "pymupdf", "pypdf": "pypdf"}
        self.model_name = "en_core_web_sm"
    def check_and_acquire(self):
        for lib, pip in self.requirements.items():
            try: importlib.import_module(lib)
            except: subprocess.check_call([sys.executable, "-m", "pip", "install", pip])
        import spacy
        if not spacy.util.is_package(self.model_name): spacy.cli.download(self.model_name)
DependencyManager().check_and_acquire()

import spacy
from ebooklib import epub
import ebooklib
from bs4 import BeautifulSoup
import fitz
from pypdf import PdfReader

try: from status_manager import StatusTracker
except ImportError:
    class StatusTracker:
        def __init__(self, n): pass
        def update(self, f, s, d=""): pass

nlp = spacy.load("en_core_web_sm")

# ==============================================================================
# SECTION 1: PRE-PROCESSING
# ==============================================================================
class TextSanitizer:
    @staticmethod
    def clean(text):
        if not text: return ""
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text) 
        text = re.sub(r'(?i)\b(fig\.?|figure)\s*(\d+)', r'[Ref: Fig \2]', text)
        lines = text.split('\n')
        clean_lines = [l for l in lines if len(l.strip()) > 3]
        return "\n".join(clean_lines)

class FileReader:
    def read(self, path):
        ext = os.path.splitext(path)[1].lower()
        if ext == '.pdf':
            try: return [page.extract_text() for page in PdfReader(path).pages]
            except: return []
        elif ext == '.epub':
            try:
                book = epub.read_epub(path); text = []
                for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
                    text.append(BeautifulSoup(item.get_body_content(), 'html.parser').get_text())
                return text
            except: return []
        elif ext == '.txt':
             with open(path, 'r', encoding='utf-8', errors='ignore') as f: return [f.read()]
        return []

# ==============================================================================
# SECTION 2: PARSERS
# ==============================================================================
class GenericParser:
    def parse(self, doc, source, images): return []

class TechnicalParser(GenericParser):
    def parse(self, doc, source, images):
        results = []
        spec_pat = r"(\d+(\.\d+)?)\s?(mm|cm|in|ft|lbs|kg)"
        for sent in doc.sents:
            if re.search(spec_pat, sent.text):
                results.append({"type": "TECHNICAL", "name": "Spec", "snippet": sent.text, "source": source})
        return results

class ReligiousParser(GenericParser):
    def parse(self, doc, source, images):
        results = []
        for sent in doc.sents:
            if "thou" in sent.text.lower() or "god" in sent.text.lower():
                results.append({"type": "RELIGIOUS", "name": "Concept", "snippet": sent.text, "source": source})
        return results

# ==============================================================================
# SECTION 3: MASTER CONTROLLER
# ==============================================================================
class UnifiedEngine:
    def __init__(self):
        self.tracker = StatusTracker("Semantic Brain")
        self.reader = FileReader()
        self.sanitizer = TextSanitizer()
        self.parsers = {
            "TECHNICAL": TechnicalParser(), "RELIGIOUS": ReligiousParser(),
            "NARRATIVE": GenericParser(), "SCIENTIFIC": GenericParser()
        }
        self.archetypes = {
            "TECHNICAL": {
                "keywords": ["fig", "plate", "mm", "ft", "step", "warning"],
                "subtypes": {
                    "MANUAL": ["tighten", "ensure", "step", "procedure", "install"],
                    "TREATISE": ["theory", "principle", "observation", "phenomenon"]
                }
            },
            "RELIGIOUS": {
                "keywords": ["god", "lord", "thou", "shalt", "sin", "soul"],
                "subtypes": {
                    "SCRIPTURE": ["verse", "chapter", "surah", "sutra", "book of"],
                    "SERMON": ["brethren", "beloved", "let us", "congregation", "message"]
                }
            }
        }

    def detect_archetypes(self, text):
        sample = text[:8000].lower()
        if not sample: return {}
        raw = {"NARRATIVE": 0, "TECHNICAL": 0, "SCIENTIFIC": 0, "RELIGIOUS": 0}
        
        # Disambiguation: Archaic vs Religious
        if "thou" in sample and any(x in sample for x in ["plough", "acre", "crop", "fig.", "plate"]):
            raw["TECHNICAL"] += 10 
        
        raw["TECHNICAL"] += sample.count("step") + sample.count("fig")
        raw["RELIGIOUS"] += sample.count("god") + sample.count("lord")
        
        total = sum(raw.values())
        if total == 0: return {"NARRATIVE": 1.0}
        return {k: v / total for k, v in raw.items()}

    def refine_archetype(self, text, primary_domain):
        if primary_domain not in self.archetypes: return primary_domain
        sample = text[:5000].lower()
        subtypes = self.archetypes[primary_domain].get("subtypes", {})
        
        best_sub = None; max_hits = 0
        for sub_name, keywords in subtypes.items():
            hits = sum(sample.count(k) for k in keywords)
            if hits > max_hits: max_hits = hits; best_sub = sub_name
        
        return f"{primary_domain}_{best_sub}" if best_sub else primary_domain

    def process_document(self, file_path, metadata):
        print(f"   [Engine] Processing {os.path.basename(file_path)}...")
        self.tracker.update(file_path, "Ingesting", "Reading")
        pages = self.reader.read(file_path)
        if not pages: return None
        
        clean_pages = [self.sanitizer.clean(p) for p in pages]
        full_text = "\n".join(clean_pages[:10])
        
        # 1. Detect Base Archetypes
        scores = self.detect_archetypes(full_text)
        primary = max(scores, key=scores.get)
        
        # 2. Refine Sub-Archetype
        subtype = self.refine_archetype(full_text, primary)
        self.tracker.update(file_path, "Analysis", f"Type: {subtype}")
        
        # 3. Active Domains (Hybrid)
        active_domains = [d for d, s in scores.items() if s > 0.15]
        if not active_domains: active_domains = [primary]

        # 4. Parse (Ensemble)
        doc = nlp(full_text[:100000])
        for domain in active_domains:
            if domain in self.parsers:
                self.parsers[domain].parse(doc, metadata.get("book"), [])

        return {
            "archetype": primary,
            "subtype": subtype,
            "mixed_domains": active_domains,
            "tags": [f"Archetype: {d.title()}" for d in active_domains] + [f"Format: {subtype.title()}"],
            "generated_summary": f"Detected {subtype.lower().replace('_', ' ')}."
        }

    def export(self, filepath):
        pass

